---
phase: 06-data-hygiene-reliability
plan: 04
type: execute
wave: 3
depends_on:
  - 06-03
files_modified:
  - dolphin/state.py
  - dolphin/sheets_sync.py
  - dolphin/tracker.py
autonomous: true

must_haves:
  truths:
    - "not_found accounts are tracked with first_seen date"
    - "Accounts not_found for 7+ consecutive days are archived"
    - "Recovered accounts (back to active) clear their not_found tracking"
  artifacts:
    - path: "dolphin/state.py"
      provides: "account_history tracking"
      contains: "account_history"
    - path: "dolphin/sheets_sync.py"
      provides: "archive_dead_accounts function"
      contains: "archive_dead_accounts"
  key_links:
    - from: "dolphin/tracker.py"
      to: "dolphin/state.py"
      via: "update_not_found_tracking call"
      pattern: "update_not_found_tracking"
    - from: "dolphin/tracker.py"
      to: "dolphin/sheets_sync.py"
      via: "archive_dead_accounts call"
      pattern: "archive_dead_accounts"
---

<objective>
Track and archive Reddit accounts that have been not_found for 7+ consecutive days.

Purpose: Dead Reddit accounts (deleted, permanently suspended) clutter the main sheet. After 7 days of not_found status, move them to Archive to keep the main view focused on active accounts.

Output: Extended state tracking for not_found duration, automatic archival after 7 days.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/06-data-hygiene-reliability/06-RESEARCH.md

# Depends on 06-03 for archive infrastructure
@.planning/phases/06-data-hygiene-reliability/06-03-SUMMARY.md

# Existing state and sync modules
@dolphin/state.py
@dolphin/sheets_sync.py
@dolphin/tracker.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend state.py for not_found duration tracking</name>
  <files>dolphin/state.py</files>
  <action>
Extend state.py to track how long accounts have been in not_found status.

1. Update load_state() to handle the new account_history field:
   - Return structure now includes: accounts, account_history, proxies, run_at
   - If account_history is missing or not a dict, default to empty dict

2. Update save_state() to include account_history:
   - Add account_history to the required structure check

3. Add new function `update_not_found_tracking`:
   ```python
   def update_not_found_tracking(
       current_accounts: dict[str, str],
       history: dict[str, dict],
       threshold_days: int = 7
   ) -> tuple[dict, list[str]]:
       """
       Track not_found duration and identify accounts for archival.

       Args:
           current_accounts: dict mapping username -> status
           history: dict mapping username -> {first_seen_not_found, consecutive_not_found_days}
           threshold_days: Number of days before archiving (default 7)

       Returns:
           Tuple of (updated_history, list_of_usernames_to_archive)
       """
   ```

   Logic:
   a. For each username in current_accounts:
      - If status == "not_found":
        - If not in history: add with first_seen_not_found = now, days = 1
        - If in history: calculate days since first_seen, update consecutive_not_found_days
        - If days >= threshold_days: add to archive list
      - If status != "not_found" and in history:
        - Remove from history (account recovered)

   b. Use timezone.utc for all datetime operations (match existing pattern)
   c. Return (updated_history, accounts_to_archive)
  </action>
  <verify>python3 -c "from dolphin.state import update_not_found_tracking; print('Import OK')"</verify>
  <done>update_not_found_tracking function exists with correct signature</done>
</task>

<task type="auto">
  <name>Task 2: Add archive_dead_accounts to sheets_sync.py</name>
  <files>dolphin/sheets_sync.py</files>
  <action>
Add function to archive accounts that have been not_found for 7+ days.

Add `archive_dead_accounts` function:
```python
def archive_dead_accounts(usernames_to_archive: list[str]) -> dict:
    """
    Archive accounts that have been not_found for threshold days.

    Args:
        usernames_to_archive: List of Reddit usernames to archive

    Returns:
        dict with "archived" count
    """
```

Implementation:
a. Connect to Google Sheets (same pattern as sync_to_sheet)
b. Get all values from main sheet (row 3 onwards)
c. Find rows where username (column B, index 1) is in usernames_to_archive
d. For each matching row:
   - Append to Archive sheet with archive_reason="dead_account_7d" and archived_at=now
e. Delete matching rows from main sheet (work backwards)
f. Return {"archived": count}

This reuses the archive infrastructure from 06-03 (same _get_or_create_archive_sheet helper).
  </action>
  <verify>python3 -c "from dolphin.sheets_sync import archive_dead_accounts; print('Import OK')"</verify>
  <done>archive_dead_accounts function exists and imports</done>
</task>

<task type="auto">
  <name>Task 3: Integrate dead account archival into tracker</name>
  <files>dolphin/tracker.py</files>
  <action>
Update tracker.py to track and archive dead accounts.

1. Add import:
   ```python
   from state import load_state, save_state, build_current_state, detect_changes, update_not_found_tracking
   from sheets_sync import sync_to_sheet, archive_stale_profiles, archive_dead_accounts
   ```

2. In the state tracking section (after detect_changes), add:
   ```python
   # Track not_found duration and archive dead accounts
   account_history = previous_state.get("account_history", {})
   updated_history, dead_accounts = update_not_found_tracking(
       current_state["accounts"],
       account_history,
       threshold_days=7,
   )
   current_state["account_history"] = updated_history

   if dead_accounts:
       logger.info(f"Archiving {len(dead_accounts)} dead account(s): {dead_accounts}")
       try:
           archive_dead_accounts(dead_accounts)
       except Exception as e:
           logger.warning(f"Failed to archive dead accounts: {e}")
   ```

3. This runs before save_state() so the updated account_history is persisted.

The flow is:
- load_state (includes account_history)
- detect_changes for alerts
- update_not_found_tracking (updates history, identifies dead accounts)
- archive_dead_accounts if any
- save_state (persists updated account_history)
  </action>
  <verify>grep -q "update_not_found_tracking" dolphin/tracker.py && grep -q "archive_dead_accounts" dolphin/tracker.py</verify>
  <done>tracker.py calls update_not_found_tracking and archive_dead_accounts</done>
</task>

</tasks>

<verification>
1. `python3 -c "from dolphin.state import update_not_found_tracking; h = {}; r = update_not_found_tracking({'test': 'not_found'}, h); print(r)"` - function works
2. `python3 -c "from dolphin.sheets_sync import archive_dead_accounts"` - imports work
3. `grep -q "account_history" dolphin/state.py` - new field present
4. `grep -q "dead_account_7d" dolphin/sheets_sync.py` - archive reason present
5. `grep -q "threshold_days=7" dolphin/tracker.py` - threshold configured
</verification>

<success_criteria>
- State file extended with account_history tracking not_found durations
- update_not_found_tracking correctly:
  - Adds new entries for newly not_found accounts
  - Increments days for persistent not_found accounts
  - Clears entries when accounts recover
  - Returns accounts meeting threshold
- archive_dead_accounts moves rows to Archive with "dead_account_7d" reason
- Tracker integrates tracking and archival in correct order
- Account history persists across runs in last_run_state.json
</success_criteria>

<output>
After completion, create `.planning/phases/06-data-hygiene-reliability/06-04-SUMMARY.md`
</output>
